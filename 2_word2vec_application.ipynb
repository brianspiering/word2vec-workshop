{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Now that we have word vectors, what can we do?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Math with words!\n",
    "\n",
    "<img src=\"http://rlv.zcache.com/math_is_awesome_poster-re24bd4726be24b82acc1d83fe7b4a8e4_cru_8byvr_512.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Types of Word Math\n",
    "----\n",
    "\n",
    "1. Distance\n",
    "2. Arithmetic\n",
    "3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "1. Distance\n",
    "---\n",
    "<br>\n",
    "<img src=\"http://blog.krecan.net/wp-content/family.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "The relationships between words can encoded as distance through the space.\n",
    "\n",
    "Words that are related will be closer than unrelate words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "Ways to measure distance\n",
    "----\n",
    "\n",
    "<img src=\"http://i1.wp.com/dataaspirant.com/wp-content/uploads/2015/04/euclidean.png?w=600\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/manhattan.png?w=600\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png?resize=610%2C468\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Read more here](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Cosine similarity is most often used in NLP.\n",
    "\n",
    "Because cosine similarity is automatically normalized. It is bounded between -1 and 1, similar to a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/math/f/3/6/f369863aa2814d6e283f859986a1574d.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Cosine values and their semantic meaning:\n",
    "\n",
    "1 : word vectors mean exactly the same  \n",
    "0 : word vectors are orthogonality (mathematically unrelated)  \n",
    "‚àí1 : word vectors mean exactly opposite  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "   \"Calculate cosine similarity between vector 1 and 2\"\n",
    "   return v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass :)\n"
     ]
    }
   ],
   "source": [
    "def test_cos_sim():\n",
    "    v1 = np.array([1, 2, 3])\n",
    "    assert cos_sim(v1, v1) == 1.0\n",
    "    \n",
    "    v2 = np.array([-1, -2, -3])\n",
    "    assert cos_sim(v1, v2) == -1.0\n",
    "    \n",
    "    v3 = np.array([0, 3])\n",
    "    v4 = np.array([4, 0])\n",
    "    assert cos_sim(v3, v4) == 0.0\n",
    "    \n",
    "    v5 = np.array([3, 45, 7, 2])\n",
    "    v6 = np.array([2, 54, 13, 15])\n",
    "    assert round(cos_sim(v5, v6), 4) == round(0.97228425171235, 4)\n",
    "    return \"tests pass :)\"\n",
    "    \n",
    "print(test_cos_sim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "Words closest to ‚ÄúSweden‚Äù\n",
    "----\n",
    "\n",
    "<img src=\"http://deeplearning4j.org/img/sweden_cosine_distance.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "2. Arithmetic: Word analogies\n",
    "---\n",
    "\n",
    "The \"Hello, world!\" of word2vec:\n",
    "> Man is to woman as king is to queen\n",
    "\n",
    "$cos(w, king) - cos(w, man) + cos(w, woman) = cos(w, queen)$\n",
    "\n",
    "![](http://multithreaded.stitchfix.com/assets/images/blog/vectors.gif)\n",
    "\n",
    "[Demo](http://rare-technologies.com/word2vec-tutorial/#app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "Different paths through word2vec space encode different relationships.\n",
    "----\n",
    "\n",
    "### Plurals\n",
    "\n",
    "![](images/plurals.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Verb Tense\n",
    "\n",
    "![](images/verb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Country-Captial\n",
    "![](images/country.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "How can you use word2vec to build data products?\n",
    "----\n",
    "\n",
    "<img src=\"https://assets.toptal.io/uploads/blog/image/827/toptal-blog-image-1423052243609.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "When I worked at an employment website, I built a recommendation engine for job seekers. The job seeker would have a resume and we would suggest jobs for them. My goal was given a current job title, suggest a \"better\" job. This would increase platform engagement.\n",
    "<br>\n",
    "<br>\n",
    "<details><summary>\n",
    "What would be next logical career move from Babysitter?\n",
    "</summary>\n",
    "A Nanny. \n",
    "<br>\n",
    "A Nanny is a Babysitter as Senior Engineer is to a Engineer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "3) Clustering\n",
    "----\n",
    "\n",
    "<img src=\"http://static1.squarespace.com/static/52165be2e4b046d1ac57778c/t/55f4a66de4b016fee4ec7595/1442096821668/left.gif?format=1500w\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Source](http://douglasduhaime.com/blog/clustering-semantic-vectors-with-python)\n",
    "\n",
    "Use your favorite!\n",
    "\n",
    "K-means is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Word2vec implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "Code\n",
    "----\n",
    "\n",
    "1. [Google‚Äôs TensorFlow](https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html)\n",
    "2. [Python‚Äôs Gensim package](https://radimrehurek.com/gensim/)  \n",
    "3. [Google‚Äôs word2vec](https://code.google.com/p/word2vec/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "Corpus (aka, data in NLP)\n",
    "----\n",
    "\n",
    "> \"Data is the world's best regularizer\"\n",
    "\n",
    "You need __a lot__ of data.\n",
    "\n",
    "100 billion words is good start üòâ. 100 million will work. 10 million is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "<br>\n",
    "<details><summary>\n",
    "How can we evaluate word2vec, especially if it is built on a custom corpus?\n",
    "</summary>\n",
    "<br>\n",
    "<br>\n",
    "Word2Vec is an unsupervised learning algorithm. Thus there is no good way to objectively evaluate the result. \n",
    "<br>\n",
    "<br>\n",
    "One possible method is to compare analogies performance with pretrained Google vectors.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Why is word2vec so popular?\n",
    "----\n",
    "\n",
    "- It is a preprocessing step that turns text into a numerical form that Deep Learning Nets and machine learning algorithms can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Dense vectors outputs (in contrast to typical NLP sparse vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can be trained and tuned on custom collection of words (corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "\n",
    "- Word2Vec is popular because it is straight forward to implement and creates dense embedding vectors.\n",
    "- Word2Vec is a _relatively_ simple neural net with 1 input layer, 1 hidden layer, and 1 output layer.\n",
    "- There are 2 common ways to represent context: \n",
    "    1. CBOW: given context, predict word\n",
    "    2. skip-gram: given word, predict context\n",
    "- After training, any vector operations can be applied to words. The most common operations are: \n",
    "    - Arithmetic (add, subtract, multiply, and divide)\n",
    "    - Distance (typically using Cosine Similarity)\n",
    "    - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Futher Study\n",
    "-----\n",
    "\n",
    "- Read [Deep or Shallow, NLP Is Breaking Out](http://dl.acm.org/citation.cfm?id=2874915)  \n",
    "- Watch [Udacity's Deep Learning course](https://www.udacity.com/course/deep-learning--ud730)\n",
    "- Watch Ali Ghodsi's word2veclectures [part 1](https://www.youtube.com/watch?v=TsEGsdVJjuA) and [part 2](https://www.youtube.com/watch?v=nuirUEmbaJU)\n",
    "- Read [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)\n",
    "- Neural Networks Demystified:\n",
    "    + [Watch](https://www.youtube.com/watch?v=5MXp9UUkSmc) \n",
    "    + [Code](http://nbviewer.ipython.org/github/stephencwelch/Neural-Networks-Demysitifed/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
