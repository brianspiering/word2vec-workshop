{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "everything2vec\n",
    "====\n",
    "\n",
    "<img src=\"images/all_the_things.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the end of this session, you should be able to:\n",
    "---\n",
    "\n",
    "- Describe the general goal and methods for sense2vec and like2vec\n",
    "- Describe the details of lda2vec\n",
    "- Describe the relationship between Bayesian prior and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Embedding  | \n",
    "|:-------:|:------:| \n",
    "| [Char2Vec](http://arxiv.org/abs/1508.06615) | Character |\n",
    "| [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) | Word | \n",
    "| [GloVe](http://www-nlp.stanford.edu/pubs/glove.pdf) | Word | \n",
    "| [Doc2Vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) | Sections of text |\n",
    "| [Image2Vec](Image2Vec) | Image |\n",
    "| [Video2Vec](https://www.dropbox.com/s/m99k5md8461xi0s/ICIP_Paper_Revised.pdf) | Video |\n",
    "\n",
    "[Source](http://datascienceassn.org/content/table-xx2vec-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Sense2vec: Revenge of POS tagging\n",
    "----\n",
    "\n",
    "<img src=\"images/duck.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec will model polysemy (the ability for the same literal characters as different human meaning) because it so may dimensions. \n",
    "\n",
    "Let's help the model by introducing priors:\n",
    "\n",
    "`(duck|NOUN)`\n",
    "\n",
    "`(duck|VERB)`\n",
    "\n",
    "<img src=\"http://www.socher.org/uploads/Main/MultipleVectorWordEmbedding.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://spacy.io/demos/sense2vec\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110decac8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://spacy.io/demos/sense2vec\",\n",
    "      width=800,\n",
    "      height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Demo](https://spacy.io/demos/sense2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: Blog post](https://spacy.io/blog/sense2vec-with-spacy)  \n",
    "[Source: Technical paper](http://arxiv.org/abs/1511.06388)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "lda2vec Overview\n",
    "----\n",
    "\n",
    "<img src=\"images/catdog_word2vec_cropped.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "word2vec review\n",
    "----\n",
    "\n",
    "1. Set up an objective function \n",
    "2. Randomly initialize vectors\n",
    "3. Do gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input vector (typically word) maximize the probablity of the output vector (typically contex):\n",
    "\n",
    "$$P(v_{OUT}|v_{IN})$$\n",
    "\n",
    "Convert to probability:  \n",
    "$$softmax(v_{IN} • v_{OUT})$$\n",
    "\n",
    "Probability of choosing 1 of N discrete items.   \n",
    "Mapping from vector space to a multinomial over words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "LDA review\n",
    "---\n",
    "\n",
    "<img src=\"http://salsahpc.indiana.edu/b649proj/images/proj3_LDA%20structure.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/Acs_esny-qQ/hqdefault.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "word2vec vs. LDA\n",
    "----\n",
    "\n",
    "<img src=\"images/compare_models.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "| algorithm | scope | prediction | numbers | visualization | density | metaphor| \n",
    "|:-------:|:------:| :------:| :------:| :------:| :------:|  :------:|\n",
    "| word2vec | local | one word predicts a nearby words | real numbers | bar chart | dense | location  |\n",
    "| LDA | global | documents predict global words | percentages that sum to 100%  | pie chart | sparse | mixture| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "lda2vec\n",
    "-----\n",
    "\n",
    "<img src=\"images/lda2vec.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "$v_{doc}$ is a mixture:  \n",
    "$v_{doc}$ = a $v_{topic1}$ + b $v_{topic2}$ + ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/doc_vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bayesian prior as a instantiation of regularization\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Review Question\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What is a Plain English definition regularization?\n",
    "</summary>\n",
    "It is a tax on complexity.\n",
    "<br>\n",
    "Small is beautiful so reduce the less helpful parts of your model\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Example of using Bayesian prior\n",
    "---\n",
    "\n",
    "In Bayesian estimation, we come up with a distribution of possible parameters using Bayes' rule: \n",
    "\n",
    "$$ P(D|θ) = \\frac{P(D|θ)P(θ)}{P(D)}$$\n",
    "\n",
    "where $P(θ)$ is known as the prior. Then, to make predictions about future events, we need to integrate over this distribution of possible $θ$.\n",
    "\n",
    "Let me give an example to make this concrete. Let's say we have a coin that comes up heads with some probability $P(θ)$. We see two heads come up. Our likelihood then becomes  $P(D|θ)=θ^2, which is clearly maximized when  $θ=1$. So, our MLE is that the coin always comes up heads, and so we predict future coins will all come up heads. We see why MLE can be a bit silly: often, it overfits the data and does not generalize well, but it is good for a first estimate.\n",
    "\n",
    "Now, let us think about the Bayesian approach. Now, let's say we initially know (our prior) that our coin has a one-half chance of having  $θ=\\frac{1}{2}$  and a one-half chance of having  $θ=1$. Let's say we observe the same two heads for coin flips. Now, let us calculate:\n",
    "\n",
    "$P(θ=1/2|D)∝P(D|θ)P(D)=1/8$   \n",
    "$P(θ=1|D)∝P(D|θ)P(D)=1/2$\n",
    "\n",
    "Normalizing, we see that we have a 1/5 chance of having a fair coin, and a 4/5 chance of having a coin that always comes up heads. So, we estimate that a new coin flip would come up heads 1/5(1/2) + 4/5(1) = 9/10 of the time. Based on our prior beliefs, we would come up with different answers, but we see that in some sense, the Bayesian estimate is more \"reasonable\" than just using MLE.\n",
    "\n",
    "This is called [Maximum a posteriori estimation (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)\n",
    "\n",
    "[Source](https://www.quora.com/Intuitively-speaking-What-is-the-difference-between-Bayesian-Estimation-and-Maximum-Likelihood-Estimation)  \n",
    "[Technical introduction to the concept](http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf)  \n",
    "[Specific application](http://papers.nips.cc/paper/2160-on-the-dirichlet-prior-and-bayesian-regularization.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Connection to regularization\n",
    "----\n",
    "\n",
    "We wieght the sample space of parameters to reduce the chance of over-fitting to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Constrained lda2vec\n",
    "----\n",
    "\n",
    "![](images/topic_lda2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "![](images/moody_lda.png)\n",
    "\n",
    "<details><summary>\n",
    "Which word2vec architecture does Chris Moody use in lda2vec?\n",
    "</summary>\n",
    "skip-gram\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "lda2vec algorithm\n",
    "----\n",
    "\n",
    "![](images/defination.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Simplex sidebar\n",
    "----\n",
    "\n",
    "Generalized notion of a triangle.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Pink_triangle_up.svg/2000px-Pink_triangle_up.svg.png\" style=\"width: 200px;\"/>\n",
    "2-simplex is a triangle\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/83/Tetrahedron.jpg\" style=\"width: 200px;\"/>\n",
    "3-simplex is a tetrahedron\n",
    "\n",
    "__REDACTED: HARD TO VISUALIZE 4 DIMENSIONS__\n",
    "4-simplex is a 5-cell\n",
    "\n",
    "---\n",
    "\n",
    "Simplex is has coordinates are nonnegative and sum to one. \n",
    "\n",
    "The 2-simplex is the triangle in  $ℝ^3$  whose vertices are at the coordinates (1, 0, 0), (0, 1, 0), (0, 0, 1).\n",
    "\n",
    "Simplex are extremely constrained and well-behaved. That makes machine learning models easier to interpret.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Simplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What other statistical concept is typically constrained to sum to 1?\n",
    "</summary>\n",
    "probabilities\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "lda2vec Executive Summary\n",
    "----\n",
    "\n",
    "![](images/punchline.png)\n",
    "\n",
    "lda2vec adds additional context, defines context as topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Segmentation\n",
    "----\n",
    "\n",
    "<img src=\"http://www.omaticsoftware.com/Portals/0/EasyDNNnews/29/iStock_000019765925_Medium.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bonus: Take Home Message\n",
    "---\n",
    "\n",
    "<img src=\"images/stat sig.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "Balance the tension between model power and interpretation.\n",
    "\n",
    "Models want to be powerful (thus complex). Business wants to understand the model.\n",
    "\n",
    "Machine readable vs. Human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "lda2vec implementation\n",
    "----\n",
    "\n",
    "[GitHub repo](https://github.com/cemoody/lda2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "like2vec\n",
    "----\n",
    "\n",
    "<img src=\"images/like2vec.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "From Galvanize & DeepMind:\n",
    "\n",
    "- Mike Tamir\n",
    "- Adam Gibson\n",
    "- Marvin Bertin\n",
    "- Michael Ulin\n",
    "- David Ott\n",
    "\n",
    "[Prerequisite reading: Adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix)\n",
    "\n",
    "[Slide deck](https://docs.google.com/presentation/d/19QDuPmxB9RzQWKXp_t3yqxCvMBSMaOQk19KNZqUUgYQ/edit#slide=id.g11bda3c091_34_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "----\n",
    "\n",
    "- sense2vec adds POS tags for embedding\n",
    "- like2vec adds adjacency matrix for embedding \n",
    "- lda2vec adds topics of document as additional context for embedding\n",
    "- Bayesian prior is a fast and an easy way to regularize\n",
    "- NLP now has the tools to move from academic navel gazing to adding business value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
